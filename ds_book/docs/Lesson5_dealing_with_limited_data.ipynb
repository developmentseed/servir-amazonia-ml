{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYe-1iFbo3dQ"
   },
   "source": [
    "# Dealing with limited data for semantic segmentation\n",
    "> Strategies for efficiently collecting more data to target specific areas of underperforming models and techniques to adopt to maximize utility of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYmAYTogOkpd"
   },
   "source": [
    "After we have evaluated how well a model has performed, we do one of two things:\n",
    "\n",
    "1. decide we are happy with how the model has performed on the validation set, and report the model performance on the test set (and validation set). Hooray!\n",
    "2. Diagnose issues with our model in terms of false positives or false negatives and make a plan for improving performance on classes that are underperforming.\n",
    "\n",
    "One of the most fundamental and high impact practices to improve model performance, particularly with deep learning, is to increase the overall size of the training dataset, focusing on classes that are underperforming. However, in remote sensing it is difficult and time consuming to acquire high quality training data labels, particularly compared to other domains where computer vision and machine learning techniques are used. \n",
    "\n",
    "Because of this unique difficulty when annotating geospatial imagery, we need to do two things:\n",
    "1. closely inspect our original labeled dataset for quality issues, such as mismatch with the imagery due to date, incorrect class labels, and incorrect label boundaries\n",
    "2. weigh the cost and benefits of annotating new labels or try other approaches to maximize our model's performance with the data we already have.\n",
    "\n",
    "Part 1 of this Lesson will describe considerations for setting up an annotation campaign, keeping in mind data quality issues that have come up during the RAMI and Terrabio projects.\n",
    "\n",
    "Part 2 will cover techniques for maximizing the performance of models trained with limited data, assuming label quality is sufficient.\n",
    "\n",
    "## Specific concepts that will be covered\n",
    "\n",
    "Part 1:\n",
    "* How to decide on a class hierarchy prior to an annotation campaign and what inputs should be made available to an annotator\n",
    "* How to efficiently annotate geospatial imagery for semantic segmentation (pixel-wise classification)\n",
    "* When it makes sense to annotate for instance segmentation (predictions are vectors) instead of semantic segmentation (predictions are rasters)\n",
    "* Choosing a sampling strategy that represents classes of interest\n",
    "\n",
    "Part 2:\n",
    "* Transfer Learning from pretrained models.  We'll use a COCO model as an example.\n",
    "* Data augmentation, or multiplying your training data with image transforms\n",
    "\n",
    "**Audience:** This post is geared towards intermediate users who are comfortable with basic machine learning concepts. \n",
    "\n",
    "**Time Estimated**: 60-120 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wLP8n9tvVwD"
   },
   "source": [
    "## Part 2: Limited Data Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYMOkBeaeuoV"
   },
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "!pip install -q rasterio==1.2.10\n",
    "!pip install -q geopandas==0.10.2\n",
    "!pip install -q git+https://github.com/tensorflow/examples.git\n",
    "!pip install -q -U tfds-nightly\n",
    "!pip install -q focal-loss\n",
    "!pip install -q tensorflow-addons==0.8.3\n",
    "#!pip install -q matplotlib==3.5 # UNCOMMENT if running on LOCAL\n",
    "!pip install -q scikit-learn==1.0.1\n",
    "!pip install -q scikit-image==0.18.3\n",
    "!pip install -q tf-explain==0.3.1\n",
    "!pip install -q segmentation_models\n",
    "!pip install -q albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_UgojhBOkpe",
    "outputId": "4551ccba-1f97-4fa5-a4d6-b0dd25f35945"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import os, glob, functools, fnmatch, io, shutil\n",
    "from zipfile import ZipFile\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import rasterio\n",
    "from rasterio import features, mask\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, losses, models\n",
    "from tensorflow.python.keras import backend as K  \n",
    "import tensorflow_addons as tfa\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "from tf_explain.callbacks.activations_visualization import ActivationsVisualizationCallback\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import skimage.io as skio\n",
    "\n",
    "import segmentation_models as sm\n",
    "from segmentation_models.losses import bce_jaccard_loss\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, Blur, HorizontalFlip, VerticalFlip,\n",
    "    Rotate, ChannelShuffle\n",
    ")\n",
    "\n",
    "# set your root directory and tiled data folders\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # this is a google colab specific command to ensure TF version 2 is used. \n",
    "    # it won't work in a regular jupyter notebook, for a regular notebook make sure you install TF version 2\n",
    "    %tensorflow_version 2.x\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    root_dir = '/content/gdrive/My Drive/servir-tf-devseed/' \n",
    "    workshop_dir = '/content/gdrive/My Drive/servir-tf-devseed-workshop'\n",
    "    print('Running on Colab')\n",
    "else:\n",
    "    root_dir = os.path.abspath(\"./data/servir-tf-devseed\")\n",
    "    workshop_dir = os.path.abspath('./servir-tf-devseed-workshop')\n",
    "    print(f'Not running on Colab, data needs to be downloaded locally at {os.path.abspath(root_dir)}')\n",
    "\n",
    "img_dir = os.path.join(root_dir,'indices/') # or os.path.join(root_dir,'images_bright/') if using the optical tiles\n",
    "label_dir = os.path.join(root_dir,'labels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEmJkl8AlE1r",
    "outputId": "bc158aa4-6697-4a7f-d80f-4453664da82c"
   },
   "outputs": [],
   "source": [
    "# go to root directory\n",
    "%cd $root_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmxfUyLrvheG"
   },
   "source": [
    "### Enabling GPU\n",
    "\n",
    "This notebook can utilize a GPU and works better if you use one. Hopefully this notebook is using a GPU, and we can check with the following code.\n",
    "\n",
    "If it's not using a GPU you can change your session/notebook to use a GPU. See [Instructions](https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=sXnDmXR7RDr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2-WXVFfvi1S",
    "outputId": "f6437983-a6c6-428e-c163-26e67f1669f5"
   },
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldG5RFPFNgOo"
   },
   "source": [
    "### Check out the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAbEMB8i87FL",
    "outputId": "8c50c8d9-2a5c-46d1-b171-8f547e46f3f2"
   },
   "outputs": [],
   "source": [
    "# Read the classes\n",
    "class_index = pd.read_csv(os.path.join(root_dir,'terrabio_classes.csv'))\n",
    "class_names = class_index.class_name.unique()\n",
    "print(class_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(workshop_dir, \"train_file_paths.csv\"))\n",
    "validate_df =  pd.read_csv(os.path.join(workshop_dir, \"validate_file_paths.csv\"))\n",
    "test_df =  pd.read_csv(os.path.join(workshop_dir, \"test_file_paths.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames = train_df[\"img_names\"]\n",
    "y_train_filenames = train_df[\"label_names\"]\n",
    "x_val_filenames = train_df[\"img_names\"]\n",
    "y_val_filenames = train_df[\"label_names\"]\n",
    "x_test_filenames = train_df[\"img_names\"]\n",
    "y_test_filenames = train_df[\"label_names\"]\n",
    "\n",
    "num_train_examples = len(x_train_filenames)\n",
    "num_val_examples = len(x_val_filenames)\n",
    "num_test_examples = len(x_test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5vOIKFXOkrh"
   },
   "source": [
    "### Setting up our Augmentations\n",
    "\n",
    "[albumentations](https://albumentations.ai/docs/examples/tensorflow-example/) is a library that contains hundreds of options for transforming images to multiply your training dataset. While each additional image may not be as additively valuable as independent samples, showing your model harder to classify copies of your existing samples can help improve your model's ability to generalize. Plus, augmentations are basically free training data!\n",
    "\n",
    "Common augmentations include brightening images, applying blur, saturation, flipping, rotating, and randomly cropping and resizing. We'll apply a few augmentations from the `albumentations` library to highlight how to set up an augmentation pipeline. This differs from coding your own augmentations, like we did in episode 3 with our horizontal flip and veritcal flip functions, saving time and lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input image shape\n",
    "img_shape = (224, 224, 3)\n",
    "# set batch size for model\n",
    "batch_size = 8\n",
    "\n",
    "transforms = Compose([\n",
    "            Rotate(limit=40),\n",
    "            HorizontalFlip(),\n",
    "            VerticalFlip(),\n",
    "            Blur(blur_limit=[3,3], p=.9),\n",
    "            ChannelShuffle(),\n",
    "        ])\n",
    "\n",
    "def aug_fn(image, img_size):\n",
    "    data = {\"image\":image}\n",
    "    aug_data = transforms(**data)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "    return aug_img\n",
    "\n",
    "# Function to augment the images and labels\n",
    "def _augment(img, label_img, img_size): \n",
    "  label_img = tf.image.resize(label_img, [224,224])\n",
    "  img = tf.image.resize(img, [224,224])\n",
    "  aug_img = tf.numpy_function(func=aug_fn, inp=[img, img_size], Tout=tf.float32)\n",
    "  return aug_img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will call our augmentation pipeline whenever we load a batch in our training or validation datasets. The augmentation pipeline that we form with `Compose()` is called in `get_baseline_dataset` during the dataset creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your data\n",
    "# Function for reading the tiles into TensorFlow tensors \n",
    "# See TensorFlow documentation for explanation of tensor: https://www.tensorflow.org/guide/tensor\n",
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair  \n",
    "  img_str = tf.io.read_file(fname)\n",
    "  img = tf.image.decode_png(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.io.read_file(label_path)\n",
    "\n",
    "  # These are png images so they return as (num_frames, h, w, c)\n",
    "  label_img = tf.image.decode_png(label_img_str, channels=1)\n",
    "  # The label image should have any values between 0 and 8, indicating pixel wise\n",
    "  # foreground class or background (0). We take the first channel only. \n",
    "  label_img = label_img[:, :, 0]\n",
    "  label_img = tf.expand_dims(label_img, axis=-1)\n",
    "  return img, label_img\n",
    "\n",
    "\n",
    "# Main function to tie all of the above four dataset processing functions together \n",
    "def get_baseline_dataset(filenames, \n",
    "                         labels,\n",
    "                         threads=5, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True):           \n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  # Map our preprocessing function to every element in our dataset, taking\n",
    "  # advantage of multithreading\n",
    "  dataset = dataset.map(functools.partial(_augment, img_size=224), num_parallel_calls=threads).prefetch(threads)\n",
    "    \n",
    "  def set_shapes(img, label, img_shape=(224,224,3), label_img_shape= (224,224,1)):\n",
    "      img.set_shape(img_shape)\n",
    "      label.set_shape(label_img_shape)\n",
    "      return img, label\n",
    "  #dataset = dataset.map(set_shapes, num_parallel_calls=threads).batch(8)\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(num_x)\n",
    "  # It's necessary to repeat our data for all epochs \n",
    "  dataset = dataset.repeat().batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset configuration for training\n",
    "train_ds = get_baseline_dataset(x_train_filenames,\n",
    "                                y_train_filenames,\n",
    "                                batch_size=batch_size)\n",
    "val_ds = get_baseline_dataset(x_val_filenames,\n",
    "                              y_val_filenames, \n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view some of our augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_image(ds):\n",
    "    image, label = next(iter(ds)) # extract 1 batch from the dataset\n",
    "    image = np.flip(image.numpy(), axis=3)\n",
    "    print(image.shape)\n",
    "    \n",
    "    fig = plt.figure(figsize=(22, 22))\n",
    "    for i in range(8):\n",
    "        ax = fig.add_subplot(2, 4, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(image[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom, more training data! Channel Shuffle presents the most extreme augmentation to the human eye. Try to adjust the Blur augmentation to create a more aggressive blurring effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_image(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a variety of augmentations that will be applied to each image in each batch, let's display an example image to see what these augmentations will look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display functions for monitoring model progress and visualizing arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]\n",
    "\n",
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "def show_predictions(image=None, mask=None, dataset=None, num=1):\n",
    "  if image is None and dataset is None:\n",
    "    # this is just for showing keras callback output. in practice this should be broken out into a different function\n",
    "    sample_image = skio.imread(f'{img_dir}/tile_terrabio_15684.png')\n",
    "    sample_mask = skio.imread(f'{label_dir}/tile_terrabio_15684.png')\n",
    "    mp = create_mask(model.predict(sample_image[tf.newaxis, ...]))\n",
    "    mpe = tf.keras.backend.eval(mp)\n",
    "    display([sample_image, sample_mask[..., tf.newaxis], mpe])\n",
    "  elif dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    mp = create_mask(model.predict(image[tf.newaxis, ...]))\n",
    "    mpe = tf.keras.backend.eval(mp)\n",
    "    display([image, mask, mpe])\n",
    "\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "callbacks = [\n",
    "    DisplayCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `segmentation_models` implementation of a U-net, since it handles downlaoding pretrained weights from a variety of sources. To set up the U-Net in a manner that is equivalent with the U-Net we made from scratch in episode 3, we need to specify the correct activation function for multi-category pixel segmentation,`softmax`, and the correct number of classes: 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "sm.set_framework('tf.keras')\n",
    "\n",
    "sm.framework()\n",
    "\n",
    "model = sm.Unet('resnet34', activation='softmax', classes = 9, encoder_weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compile our model with the same optimizer, loss function, and accuracy metrics from Lesson 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "              loss=SparseCategoricalFocalLoss(gamma=2, from_logits=True), #tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy', sm.metrics.iou_score])\n",
    "\n",
    "EPOCHS=4\n",
    "model_history = model.fit(\n",
    "   train_ds,\n",
    "   epochs=EPOCHS,\n",
    "   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
    "   validation_data=val_ds,\n",
    "   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
    "   callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can view the model's loss plot and compare to Lesson 3. Was there an improvement? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "epochs = range(EPOCHS)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Indices_DeepLearning_Crop_Segmentation_112021.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:servir]",
   "language": "python",
   "name": "conda-env-servir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
