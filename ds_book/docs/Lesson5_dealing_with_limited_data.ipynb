{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYe-1iFbo3dQ"
   },
   "source": [
    "# Dealing with limited data for semantic segmentation\n",
    "> Strategies for efficiently collecting more data to target specific areas of underperforming models and techniques to adopt to maximize utility of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYmAYTogOkpd"
   },
   "source": [
    "After we have evaluated how well a model has performed, we do one of two things:\n",
    "\n",
    "1. decide we are happy with how the model has performed on the validation set, and report the model performance on the test set (and validation set). Hooray!\n",
    "2. Diagnose issues with our model in terms of false positives or false negatives and make a plan for improving performance on classes that are underperforming.\n",
    "\n",
    "One of the most fundamental and high impact practices to improve model performance, particularly with deep learning, is to increase the overall size of the training dataset, focusing on classes that are underperforming. However, in remote sensing it is difficult and time consuming to acquire high quality training data labels, particularly compared to other domains where computer vision and machine learning techniques are used. \n",
    "\n",
    "Because of this unique difficulty when annotating geospatial imagery, we need to do two things:\n",
    "1. closely inspect our original labeled dataset for quality issues, such as mismatch with the imagery due to date, incorrect class labels, and incorrect label boundaries\n",
    "2. weigh the cost and benefits of annotating new labels or try other approaches to maximize our model's performance with the data we already have.\n",
    "\n",
    "Part 1 of this Lesson will describe considerations for setting up an annotation campaign, keeping in mind data quality issues that have come up during the RAMI and Terrabio projects.\n",
    "\n",
    "Part 2 will cover techniques for maximizing the performance of models trained with limited data, assuming label quality is sufficient.\n",
    "\n",
    "## Specific concepts that will be covered\n",
    "\n",
    "Part 1:\n",
    "* How to decide on a class hierarchy prior to an annotation campaign and what inputs should be made available to an annotator\n",
    "* How to efficiently annotate geospatial imagery for semantic segmentation (pixel-wise classification)\n",
    "* When it makes sense to annotate for instance segmentation (predictions are vectors) instead of semantic segmentation (predictions are rasters)\n",
    "* Choosing a sampling strategy that represents classes of interest\n",
    "\n",
    "Part 2:\n",
    "* Transfer Learning from pretrained models.  We'll use a COCO model as an example.\n",
    "* Data augmentation, or multiplying your training data with image transforms\n",
    "\n",
    "**Audience:** This post is geared towards intermediate users who are comfortable with basic machine learning concepts. \n",
    "\n",
    "**Time Estimated**: 60-120 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wLP8n9tvVwD"
   },
   "source": [
    "## Part 2: Limited Data Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYMOkBeaeuoV"
   },
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "!pip install -q rasterio==1.2.10\n",
    "!pip install -q geopandas==0.10.2\n",
    "!pip install -q git+https://github.com/tensorflow/examples.git\n",
    "!pip install -q -U tfds-nightly\n",
    "!pip install -q focal-loss\n",
    "!pip install -q tensorflow-addons==0.8.3\n",
    "#!pip install -q matplotlib==3.5 # UNCOMMENT if running on LOCAL\n",
    "!pip install -q scikit-learn==1.0.1\n",
    "!pip install -q scikit-image==0.18.3\n",
    "!pip install -q tf-explain==0.3.1\n",
    "!pip install segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_UgojhBOkpe",
    "outputId": "4551ccba-1f97-4fa5-a4d6-b0dd25f35945"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import os, glob, functools, fnmatch, io, shutil\n",
    "from zipfile import ZipFile\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import rasterio\n",
    "from rasterio import features, mask\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, losses, models\n",
    "from tensorflow.python.keras import backend as K  \n",
    "import tensorflow_addons as tfa\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "from tf_explain.callbacks.activations_visualization import ActivationsVisualizationCallback\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import skimage.io as skio\n",
    "\n",
    "# set your root directory and tiled data folders\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # this is a google colab specific command to ensure TF version 2 is used. \n",
    "    # it won't work in a regular jupyter notebook, for a regular notebook make sure you install TF version 2\n",
    "    %tensorflow_version 2.x\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    root_dir = '/content/gdrive/My Drive/servir-tf-devseed/' \n",
    "    workshop_dir = '/content/gdrive/My Drive/servir-tf-devseed-workshop'\n",
    "    print('Running on Colab')\n",
    "else:\n",
    "    root_dir = os.path.abspath(\"./data/servir-tf-devseed\")\n",
    "    workshop_dir = os.path.abspath('./servir-tf-devseed-workshop')\n",
    "    print(f'Not running on Colab, data needs to be downloaded locally at {os.path.abspath(root_dir)}')\n",
    "\n",
    "img_dir = os.path.join(root_dir,'indices/') # or os.path.join(root_dir,'images_bright/') if using the optical tiles\n",
    "label_dir = os.path.join(root_dir,'labels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEmJkl8AlE1r",
    "outputId": "bc158aa4-6697-4a7f-d80f-4453664da82c"
   },
   "outputs": [],
   "source": [
    "# go to root directory\n",
    "%cd $root_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmxfUyLrvheG"
   },
   "source": [
    "### Enabling GPU\n",
    "\n",
    "This notebook can utilize a GPU and works better if you use one. Hopefully this notebook is using a GPU, and we can check with the following code.\n",
    "\n",
    "If it's not using a GPU you can change your session/notebook to use a GPU. See [Instructions](https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=sXnDmXR7RDr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2-WXVFfvi1S",
    "outputId": "f6437983-a6c6-428e-c163-26e67f1669f5"
   },
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldG5RFPFNgOo"
   },
   "source": [
    "### Check out the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAbEMB8i87FL",
    "outputId": "8c50c8d9-2a5c-46d1-b171-8f547e46f3f2"
   },
   "outputs": [],
   "source": [
    "# Read the classes\n",
    "class_index = pd.read_csv(os.path.join(root_dir,'terrabio_classes.csv'))\n",
    "class_names = class_index.class_name.unique()\n",
    "print(class_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(workshop_dir, \"train_file_paths.csv\"))\n",
    "validate_df =  pd.read_csv(os.path.join(workshop_dir, \"validate_file_paths.csv\"))\n",
    "test_df =  pd.read_csv(os.path.join(workshop_dir, \"test_file_paths.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames = train_df[\"img_names\"]\n",
    "y_train_filenames = train_df[\"label_names\"]\n",
    "x_val_filenames = train_df[\"img_names\"]\n",
    "y_val_filenames = train_df[\"label_names\"]\n",
    "x_test_filenames = train_df[\"img_names\"]\n",
    "y_test_filenames = train_df[\"label_names\"]\n",
    "\n",
    "num_train_examples = len(x_train_filenames)\n",
    "num_val_examples = len(x_val_filenames)\n",
    "num_test_examples = len(x_test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5vOIKFXOkrh"
   },
   "source": [
    "### Loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, you can load the model from the saved version\n",
    "load_from_checkpoint = True\n",
    "if load_from_checkpoint == True:\n",
    "  save_model_path = os.path.join(root_dir,'indices_model_out_lr00001_batchs8_ep50_nopretrain_focalloss/')\n",
    "  model = tf.keras.models.load_model(save_model_path, custom_objects={'loss': SparseCategoricalFocalLoss(gamma=2, from_logits=True)})\n",
    "else:\n",
    "  print(\"inferencing from in memory model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "\n",
    "sm.set_framework('tf.keras')\n",
    "\n",
    "sm.framework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Unet('resnet34', encoder_weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "\n",
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "# set input image shape\n",
    "img_shape = (224, 224, 3)\n",
    "# set batch size for model\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your data\n",
    "# Function for reading the tiles into TensorFlow tensors \n",
    "# See TensorFlow documentation for explanation of tensor: https://www.tensorflow.org/guide/tensor\n",
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair  \n",
    "  img_str = tf.io.read_file(fname)\n",
    "  img = tf.image.decode_png(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.io.read_file(label_path)\n",
    "\n",
    "  # These are png images so they return as (num_frames, h, w, c)\n",
    "  label_img = tf.image.decode_png(label_img_str, channels=1)\n",
    "  # The label image should have any values between 0 and 8, indicating pixel wise\n",
    "  # foreground class or background (0). We take the first channel only. \n",
    "  label_img = label_img[:, :, 0]\n",
    "  label_img = tf.expand_dims(label_img, axis=-1)\n",
    "  return img, label_img\n",
    "\n",
    "# Function to augment the data with horizontal flip\n",
    "def flip_img_h(horizontal_flip, tr_img, label_img):\n",
    "  if horizontal_flip:\n",
    "    flip_prob = tf.random.uniform([], 0.0, 1.0)\n",
    "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
    "                                lambda: (tr_img, label_img))\n",
    "  return tr_img, label_img\n",
    "\n",
    "# Function to augment the data with vertical flip\n",
    "def flip_img_v(vertical_flip, tr_img, label_img):\n",
    "  if vertical_flip:\n",
    "    flip_prob = tf.random.uniform([], 0.0, 1.0)\n",
    "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                lambda: (tf.image.flip_up_down(tr_img), tf.image.flip_up_down(label_img)),\n",
    "                                lambda: (tr_img, label_img))\n",
    "  return tr_img, label_img\n",
    "\n",
    "# Function to augment the images and labels\n",
    "def _augment(img,\n",
    "             label_img,\n",
    "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
    "             #scale=1,  # Scale image e.g. 1 / 255.\n",
    "             horizontal_flip=False,\n",
    "             vertical_flip=False): \n",
    "  if resize is not None:\n",
    "    # Resize both images\n",
    "    label_img = tf.image.resize(label_img, resize)\n",
    "    img = tf.image.resize(img, resize)\n",
    "  \n",
    "  img, label_img = flip_img_h(horizontal_flip, img, label_img)\n",
    "  img, label_img = flip_img_v(vertical_flip, img, label_img)\n",
    "  img = tf.cast(img, tf.float32) # * scale  #tf.to_float(img) * scale \n",
    "  #label_img = tf.cast(label_img, tf.float32) * scale\n",
    "  #print(\"tensor: \", tf.unique(tf.keras.backend.print_tensor(label_img)))\n",
    "  return img, label_img\n",
    "\n",
    "# Main function to tie all of the above four dataset processing functions together \n",
    "def get_baseline_dataset(filenames, \n",
    "                         labels,\n",
    "                         preproc_fn=functools.partial(_augment),\n",
    "                         threads=5, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True):           \n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  # Map our preprocessing function to every element in our dataset, taking\n",
    "  # advantage of multithreading\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
    "    assert batch_size == 1, \"Batching images must be of the same size\"\n",
    "\n",
    "  dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
    "  \n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(num_x)\n",
    "  \n",
    "  \n",
    "  # It's necessary to repeat our data for all epochs \n",
    "  dataset = dataset.repeat().batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset configuration for training\n",
    "tr_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    #'scale': 1 / 255.,\n",
    "    'horizontal_flip': True,\n",
    "    'vertical_flip': True,\n",
    "}\n",
    "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)\n",
    "train_ds = get_baseline_dataset(x_train_filenames,\n",
    "                                y_train_filenames,\n",
    "                                preproc_fn=tr_preprocessing_fn,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "# dataset configuration for validation\n",
    "val_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    #'scale': 1 / 255.,\n",
    "}\n",
    "val_preprocessing_fn = functools.partial(_augment, **val_cfg)\n",
    "val_ds = get_baseline_dataset(x_val_filenames,\n",
    "                              y_val_filenames, \n",
    "                              preproc_fn=val_preprocessing_fn,\n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display functions for monitoring model progress and visualizing arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]\n",
    "\n",
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "def show_predictions(image=None, mask=None, dataset=None, num=1):\n",
    "  if image is None and dataset is None:\n",
    "    # this is just for showing keras callback output. in practice this should be broken out into a different function\n",
    "    sample_image = skio.imread(f'{img_dir}/tile_terrabio_15684.png')\n",
    "    sample_mask = skio.imread(f'{label_dir}/tile_terrabio_15684.png')\n",
    "    mp = create_mask(model.predict(sample_image[tf.newaxis, ...]))\n",
    "    mpe = tf.keras.backend.eval(mp)\n",
    "    display([sample_image, sample_mask[..., tf.newaxis], mpe])\n",
    "  elif dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    mp = create_mask(model.predict(image[tf.newaxis, ...]))\n",
    "    mpe = tf.keras.backend.eval(mp)\n",
    "    display([image, mask, mpe])\n",
    "\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "callbacks = [\n",
    "    DisplayCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models.losses import bce_jaccard_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = sm.Unet(BACKBONE, encoder_weights='imagenet', encoder_freeze=True)\n",
    "model.compile(\n",
    "    optimizer=\"Adam\",\n",
    "    loss=bce_jaccard_loss,\n",
    "    metrics=['accuracy', sm.metrics.iou_score],\n",
    ")\n",
    "\n",
    "# fit model\n",
    "# if you use data generator use model.fit_generator(...) instead of model.fit(...)\n",
    "# more about `fit_generator` here: https://keras.io/models/sequential/#fit_generator\n",
    "EPOCHS=4\n",
    "model_history = model.fit(\n",
    "   train_ds,\n",
    "   epochs=EPOCHS,\n",
    "   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
    "   validation_data=val_ds,\n",
    "   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
    "   callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "epochs = range(EPOCHS)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Indices_DeepLearning_Crop_Segmentation_112021.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:servir]",
   "language": "python",
   "name": "conda-env-servir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
