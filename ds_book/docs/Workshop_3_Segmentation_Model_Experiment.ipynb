{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb938174-36e9-49c0-831c-4e3f1b5845f5",
   "metadata": {},
   "source": [
    "# Lesson 3: Training a U-Net Segmentation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab9236-0663-4849-9dea-7e52a68790e7",
   "metadata": {},
   "source": [
    "### Getting set up with the data\n",
    "\n",
    "Create drive shortcuts of the tiled imagery to your own My Drive Folder by Right-Clicking on the Shared folder `terrabio`. Then, this folder will be available at the following path that is accessible with the google.colab `drive` module: `'/content/gdrive/My Drive/servir-tf/terrabio/'`\n",
    "\n",
    "We'll be working witht he following folders in the `tiled` folder:\n",
    "```\n",
    "tiled/\n",
    "├── images/\n",
    "├── images_bright/\n",
    "├── indices/\n",
    "├── indices_800/\n",
    "├── labels/\n",
    "└── labels_800/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6a58b-64a1-47b0-9d33-39a233dc7100",
   "metadata": {},
   "source": [
    "pip version 21.1.3 and python 3.7 is required or else tensorflow add ons will not install. python 3.8 won't work because tf add ons version is too old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b65607-9e73-4ab6-bb20-256946c46179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "758fee28-b296-4769-9446-bc1ec5bb78d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 21.1.3 from /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages/pip (python 3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ac5ad8-4d25-4da9-88e9-e1fd5f3a24f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip==21.1.3 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (21.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pip==21.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701d2d44-826b-42d5-b539-0e487e095909",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/examples.git\n",
      "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-w80gv59t\n",
      "  Running command git clone -q https://github.com/tensorflow/examples.git /tmp/pip-req-build-w80gv59t\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorflow-examples===6e73f65d7883e301a4115b65988a4f893c47430d-) (1.16.0)\n",
      "Building wheels for collected packages: tensorflow-examples\n",
      "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorflow-examples: filename=tensorflow_examples-6e73f65d7883e301a4115b65988a4f893c47430d_-py3-none-any.whl size=268415 sha256=d6d85e6e9bdbb8a702db0164e2e285e8ce7a5645f0c8505553d0132c11d7a05b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g3khc3kp/wheels/eb/19/50/2a4363c831fa12b400af86325a6f26ade5d2cdc5b406d552ca\n",
      "\u001b[33m  WARNING: Built wheel for tensorflow-examples is invalid: Metadata 1.2 mandates PEP 440 version, but '6e73f65d7883e301a4115b65988a4f893c47430d-' is not\u001b[0m\n",
      "Failed to build tensorflow-examples\n",
      "Installing collected packages: absl-py, tensorflow-examples\n",
      "    Running setup.py install for tensorflow-examples ... \u001b[?25ldone\n",
      "\u001b[33m  DEPRECATION: tensorflow-examples was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
      "\u001b[?25hSuccessfully installed absl-py-1.0.0 tensorflow-examples-6e73f65d7883e301a4115b65988a4f893c47430d-\n",
      "Collecting tfds-nightly\n",
      "  Using cached tfds_nightly-4.4.0.dev202111150106-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: absl-py in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tfds-nightly) (1.0.0)\n",
      "Collecting protobuf>=3.12.2\n",
      "  Downloading protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting requests>=2.19.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: numpy in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tfds-nightly) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tfds-nightly) (4.0.0)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: six in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tfds-nightly) (1.16.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.4.0-py3-none-any.whl (48 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (2021.10.8)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from importlib-resources->tfds-nightly) (3.6.0)\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: future, promise, termcolor\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=de4f6d6ed4e67c94f941603c8e857e58834d12aea7d0cadc983d82b6ec279d41\n",
      "  Stored in directory: /home/rave/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=df86bbdea780e323d98ef49300951c2e1f1c3ccff2a7334515960b359b283cc7\n",
      "  Stored in directory: /home/rave/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=b30191c3465bf2256391f6b0f78af80d37cdc5e734e181cdbed3e9b63d780ede\n",
      "  Stored in directory: /home/rave/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built future promise termcolor\n",
      "Installing collected packages: protobuf, urllib3, idna, googleapis-common-protos, charset-normalizer, absl-py, tqdm, termcolor, tensorflow-metadata, requests, promise, importlib-resources, future, dill, tfds-nightly\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.0.0\n",
      "    Uninstalling absl-py-1.0.0:\n",
      "      Successfully uninstalled absl-py-1.0.0\n",
      "Successfully installed absl-py-0.12.0 charset-normalizer-2.0.7 dill-0.3.4 future-0.18.2 googleapis-common-protos-1.53.0 idna-3.3 importlib-resources-5.4.0 promise-2.3 protobuf-3.19.1 requests-2.26.0 tensorflow-metadata-1.4.0 termcolor-1.1.0 tfds-nightly-4.4.0.dev202111150106 tqdm-4.62.3 urllib3-1.26.7\n",
      "Collecting focal-loss\n",
      "  Using cached focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
      "Collecting tensorflow>=2.2\n",
      "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 489.6 MB 71 kB/s s eta 0:00:01     |█████████████████████▉          | 334.6 MB 21.7 MB/s eta 0:00:08\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.41.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 56.5 MB/s eta 0:00:01     |█████████████████████████████▉  | 3.7 MB 56.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 22.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (3.19.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 46.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 59.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.5.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.21.4)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.4 MB 32.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.32.0 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (0.37.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (4.0.0)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[K     |████████████████████████████████| 463 kB 25.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (1.16.0)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorflow>=2.2->focal-loss) (0.12.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (58.0.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2.26.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 52.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (4.8.2)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2.0.7)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.6.0)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, cached-property, wrapt, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow, focal-loss\n",
      "Successfully installed astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 flatbuffers-2.0 focal-loss-0.0.7 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.1 h5py-3.5.0 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 werkzeug-2.0.2 wrapt-1.13.3\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /home/rave/miniconda3/envs/servir/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.2 MB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=1c417af6ce7cc8612e17620264393b4de45ba4147ce54f1fc617ebde6b1073af\n",
      "  Stored in directory: /home/rave/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.1 scipy-1.7.2 sklearn-0.0 threadpoolctl-3.0.0\n",
      "Collecting tensorflow-addons==0.8.3\n",
      "  Downloading tensorflow_addons-0.8.3-cp37-cp37m-manylinux2010_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard\n",
      "  Downloading typeguard-2.13.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.8.3 typeguard-2.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q rasterio\n",
    "!pip install -q geopandas\n",
    "!pip install git+https://github.com/tensorflow/examples.git\n",
    "!pip install -U tfds-nightly\n",
    "!pip install focal-loss\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "!pip install sklearn # on colab already but maybe not on local\n",
    "!pip install tensorflow-addons==0.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e30cc0ef-4b0e-4458-ab37-a1b5be5e550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, functools, fnmatch\n",
    "from zipfile import ZipFile\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import rasterio\n",
    "from rasterio import features, mask, windows\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, losses, models\n",
    "from tensorflow.python.keras import backend as K  \n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c11ff6-8b7c-41a3-ae54-b5996db1ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    root_dir = '/content/gdrive/My Drive/servir-tf/terrabio/' \n",
    "    print('Running on CoLab')\n",
    "else:\n",
    "    root_dir = './data/' \n",
    "    print(f'Not running on CoLab, data needs to be downloaded locally at {os.path.abspath(root_dir)}')\n",
    "\n",
    "img_dir = root_dir+'tiled/indices/' # or root_dir+'tiled/images_bright/' if using the optical tiles\n",
    "label_dir = root_dir+'tiled/labels/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0bac8-88ca-4667-8bd0-6e76af1e2b3b",
   "metadata": {},
   "source": [
    "### Enabling GPU\n",
    "\n",
    "This notebook can utilize a GPU and works better if you use one. Hopefully this notebook is using a GPU, and we can check with the following code.\n",
    "\n",
    "If it's not using a GPU you can change your session/notebook to use a GPU. See [Instructions](https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=sXnDmXR7RDr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ed7662c-1b6b-4da7-b67c-36718425876e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 15:53:40.520590: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-15 15:53:40.551667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 15:53:40.575217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 15:53:40.575674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 15:53:40.940537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 15:53:40.940924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 15:53:40.941252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 15:53:40.941600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 9731 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b1030-4a9a-401d-bf7e-d3a7e86de771",
   "metadata": {},
   "source": [
    "As a recap, let's check out the classes we are workign with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a209ba7d-ea46-48d8-903e-605f70381222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_id   class_name\n",
      "0         0   Background\n",
      "1         1     Bushland\n",
      "2         2      Pasture\n",
      "3         3        Roads\n",
      "4         4        Cocoa\n",
      "5         5   Tree cover\n",
      "6         6    Developed\n",
      "7         7        Water\n",
      "8         8  Agriculture\n"
     ]
    }
   ],
   "source": [
    "# Read the classes\n",
    "class_index = pd.read_csv(root_dir+'terrabio_classes.csv')\n",
    "class_names = class_index.class_name.unique()\n",
    "print(class_index) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cade3f9-7332-46ce-bf91-4da241467fc9",
   "metadata": {},
   "source": [
    "### Read the image files and label files into tensorflow dataset Python objects\n",
    "\n",
    "Now we will compile the spectral index image and label tiles into training, validation, and test datasets for use with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a44d8cb3-2e6a-4012-964a-d8a2d5485026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lists of image and label tile pairs for training and testing\n",
    "\n",
    "def get_train_test_lists(imdir, lbldir):\n",
    "    imgs = glob.glob(imdir+\"/*.png\")\n",
    "    dset_list = []\n",
    "    for img in imgs:\n",
    "        filename_split = os.path.splitext(img) \n",
    "        filename_zero, fileext = filename_split\n",
    "        basename = os.path.basename(filename_zero)\n",
    "        dset_list.append(basename)\n",
    "    x_filenames = []\n",
    "    y_filenames = []\n",
    "    for img_id in dset_list:\n",
    "        x_filenames.append(os.path.join(imdir, \"{}.png\".format(img_id)))\n",
    "        y_filenames.append(os.path.join(lbldir, \"{}.png\".format(img_id)))\n",
    "    print(\"number of images: \", len(dset_list))\n",
    "    print(\"number of labels: \", len(y_filenames))\n",
    "    return dset_list, x_filenames, y_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55219775-5609-47c0-82e4-7430e5ca1f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images:  37350\n",
      "number of labels:  37350\n"
     ]
    }
   ],
   "source": [
    "train_list, x_train_filenames, y_train_filenames = get_train_test_lists(img_dir, label_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc6d55-ecce-48d4-804c-65f15c0a99c8",
   "metadata": {},
   "source": [
    "Let's check for the proportion of background tiles. This takes a while. So we can skip by loading from saved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fa2b964-e9e6-4f15-ab44-0189e4873602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of background training images:  36489\n"
     ]
    }
   ],
   "source": [
    "skip = False\n",
    "\n",
    "if not skip:\n",
    "    background_list_train = []\n",
    "    for i in train_list: \n",
    "        # read in each labeled images\n",
    "        #print(label_dir+\"{}.png\".format(i))\n",
    "        img = np.array(Image.open(label_dir+\"{}.png\".format(i))) \n",
    "        # check if no values in image are greater than zero (background value)\n",
    "        if img.max()==0:\n",
    "            background_list_train.append(i)\n",
    "    print(\"number of background training images: \", len(background_list_train))\n",
    "    with open(os.path.join(root_dir,'background_list_train.txt'), 'w') as f:\n",
    "        for item in background_list_train:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "else:\n",
    "    background_list_train = [line.strip() for line in open(root_dir+\"background_list_train.txt\", 'r')]\n",
    "    print(\"number of background training images: \", len(background_list_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1132518-6916-4d49-9353-3ac76ef60d5b",
   "metadata": {},
   "source": [
    "We will keep only 10% of the total amount of background images. Too many background tiles can cause a form of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dfd9e-ec46-4196-824f-7d1c1eab1814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32840.1\n",
      "Remaining trianing set size : 4510\n"
     ]
    }
   ],
   "source": [
    "background_removal = len(background_list_train) * 0.9\n",
    "print(background_removal)\n",
    "train_list_clean = [y for y in train_list if y not in background_list_train[0:int(background_removal)]]\n",
    "\n",
    "x_train_filenames = []\n",
    "y_train_filenames = []\n",
    "for img_id in train_list_clean: \n",
    "    x_train_filenames.append(os.path.join(img_dir, \"{}.png\".format(img_id)))\n",
    "    y_train_filenames.append(os.path.join(label_dir, \"{}.png\".format(img_id)))\n",
    "    \n",
    "print(f\"Remaining trianing set size : {len(train_list_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee52e16-e705-458f-8d50-82fab46894f1",
   "metadata": {},
   "source": [
    "Now that we have our set of files we want to use for developing our model, we need to split them into three sets: \n",
    "* the Training set for the model to learn from\n",
    "* the validation set that allows us to evaluate models and make decisions to change models\n",
    "* and the test set that we will use to communicate the results of the best performing model (as determined by the validation set)\n",
    "\n",
    "We will split index tiles and label tiles into train, validation and test sets: 70%, 20% and 10%, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34709ca2-b862-4533-989b-65daa701e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2209\n",
      "Number of validation examples: 635\n",
      "Number of test examples: 313\n"
     ]
    }
   ],
   "source": [
    "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = train_test_split(x_train_filenames, y_train_filenames, test_size=0.3, random_state=42)\n",
    "x_val_filenames, x_test_filenames, y_val_filenames, y_test_filenames = train_test_split(x_val_filenames, y_val_filenames, test_size=0.33, random_state=42)\n",
    "\n",
    "num_train_examples = len(x_train_filenames)\n",
    "num_val_examples = len(x_val_filenames)\n",
    "num_test_examples = len(x_test_filenames)\n",
    "\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of validation examples: {}\".format(num_val_examples))\n",
    "print(\"Number of test examples: {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2d428ee-268b-4e80-83aa-e90337110c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09914475768134305"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f81b9-0308-46dd-874a-12f343e6964b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:servir]",
   "language": "python",
   "name": "conda-env-servir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
