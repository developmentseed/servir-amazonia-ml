
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction to machine learning, neural networks and deep learning &#8212; Deep learning with TensorFlow</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/ds.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Introduction to TensorFlow and Keras" href="Lesson1b_Intro_TensorFlow_Keras.html" />
    <link rel="prev" title="Deep Learning with TensorFlow: Tutorials for modeling in the Amazon" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/ds.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep learning with TensorFlow</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to machine learning, neural networks and deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Lesson1b_Intro_TensorFlow_Keras.html">
   Introduction to TensorFlow and Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Lesson2a_get_planet_NICFI.html">
   Access and mosaic Planet NICFI monthly basemaps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Lesson2b_prep_data_ML_segmentation.html">
   Process dataset for use with deep learning segmentation network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Lesson3_deeplearning_crop_segmentation.html">
   Semantic segmentation with deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Lesson4_evaluation.html">
   Evaluating Semantic Segmentation Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Lesson5_dealing_with_limited_data.html">
   Dealing with limited data for semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/Lesson1a_Intro_ML_NN_DL.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/developmentseed/servir-amazonia-ml/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/developmentseed/servir-amazonia-ml//issues/new?title=Issue%20on%20page%20%2Fdocs/Lesson1a_Intro_ML_NN_DL.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/developmentseed/servir-amazonia-ml/edit/main/ds_book/docs/Lesson1a_Intro_ML_NN_DL.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/developmentseed/servir-amazonia-ml/main?urlpath=tree/ds_book/docs/Lesson1a_Intro_ML_NN_DL.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/developmentseed/servir-amazonia-ml/blob/main/ds_book/docs/Lesson1a_Intro_ML_NN_DL.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objectives">
   Objectives
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-machine-learning">
     What is Machine Learning?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-neural-networks">
     What are Neural Networks?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-are-convolutional-neural-networks">
       What are Convolutional Neural Networks?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-a-kernel-filter">
       What is a kernel/filter?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-stride">
       What is stride?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-a-convolution-operation">
       What is a convolution operation?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#convolution-operation-using-3d-filter">
       Convolution operation using 3D filter
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-padding">
       What is padding?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-deep-learning">
     What is Deep Learning?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-and-testing-data">
       Training and Testing Data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#forward-and-backward-propagation-hyper-parameters-and-learnable-parameters">
       Forward and backward propagation, hyper-parameters, and learnable parameters
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#common-deep-learning-algorithms-for-computer-vision">
       Common Deep Learning Algorithms for Computer Vision
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#semantic-segmentation">
       Semantic Segmentation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#u-net-segmentation-architecture">
       U-Net Segmentation Architecture
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="introduction-to-machine-learning-neural-networks-and-deep-learning">
<h1>Introduction to machine learning, neural networks and deep learning<a class="headerlink" href="#introduction-to-machine-learning-neural-networks-and-deep-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="objectives">
<h2>Objectives<a class="headerlink" href="#objectives" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Understand the fundamental goals of machine learning and a bit of the field’s history</p></li>
<li><p>Gain familiarity with the mechanics of a neural network, convolutional neural networks, and the U-Net architecture in particular</p></li>
<li><p>Discuss considerations for choosing a deep learning architecture for a particular problem</p></li>
</ul>
<div class="section" id="what-is-machine-learning">
<h3>What is Machine Learning?<a class="headerlink" href="#what-is-machine-learning" title="Permalink to this headline">¶</a></h3>
<p>Machine learning (ML) is a subset of artificial intelligence (AI), which in broad terms, is defined as the ability of a machine to simulate intelligent human behavior.</p>
<div class="figure align-default" id="ai-ml-dl-fig">
<a class="reference internal image-reference" href="https://human-centered.ai/wordpress/wp-content/uploads/2017/11/Deep-Learning-subset-of-Machine-Learning-subset-of-Artificial-Intelligence.jpg"><img alt="https://human-centered.ai/wordpress/wp-content/uploads/2017/11/Deep-Learning-subset-of-Machine-Learning-subset-of-Artificial-Intelligence.jpg" src="https://human-centered.ai/wordpress/wp-content/uploads/2017/11/Deep-Learning-subset-of-Machine-Learning-subset-of-Artificial-Intelligence.jpg" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text"><a class="reference external" href="https://www.frwebs.top/products.aspx?cname=difference+between+ml+dl+and+ai&amp;cid=7">AI, ML, DL</a>.</span><a class="headerlink" href="#ai-ml-dl-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Compared to traditional programming, ML offers:</p>
<ol class="simple">
<li><p>time savings on behalf of the human programmer,</p></li>
<li><p>time savings on behalf of a human manual interpreter,</p></li>
<li><p>reduction of human error,</p></li>
<li><p>scalable decision making</p></li>
</ol>
<p>ML requires good quality data, and a lot of it, to recognize key patterns and features.</p>
<p>Humans still have a role in this process, by way of supplying the model with data and choosing algorithms and parameters.</p>
<p>There are several subcategories of machine learning:</p>
<ol class="simple">
<li><p><strong>Supervised machine learning</strong> involves training a model with labeled data sets that explicitly give examples of predictive features and their target attribute(s).</p></li>
<li><p><strong>Unsupervised machine learning</strong> involves tasking a model to search for patterns in data without the guidance of labels.</p></li>
</ol>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>There are also some problems where machine learning is uniquely equipped to learn insights and make decisions when a human might not, such as drawing relationships from combined spectral indices in a complex terrain.</p>
</div>
</div>
<div class="section" id="what-are-neural-networks">
<h3>What are Neural Networks?<a class="headerlink" href="#what-are-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>Artificial neural networks (ANNs) are a specific, biologically-inspired class of machine learning algorithms. They are modeled after the structure and function of the human brain.</p>
<div class="figure align-default" id="neuron-fig">
<a class="reference internal image-reference" href="../_images/neuron-structure.jpg"><img alt="../_images/neuron-structure.jpg" src="../_images/neuron-structure.jpg" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Biological neuron (from <a class="reference external" href="https://training.seer.cancer.gov/anatomy/nervous/tissue.html">https://training.seer.cancer.gov/anatomy/nervous/tissue.html</a>).</span><a class="headerlink" href="#neuron-fig" title="Permalink to this image">¶</a></p>
</div>
<p>ANNs are essentially programs that make decisions by weighing the evidence and responding to feedback. By varying the input data, types of parameters and their values, we can get different models of decision-making.</p>
<div class="figure align-default" id="neuralnet-basic-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/1100/1*x6KWjKTOBhUYL0MRX4M3oQ.png"><img alt="https://miro.medium.com/max/1100/1*x6KWjKTOBhUYL0MRX4M3oQ.png" src="https://miro.medium.com/max/1100/1*x6KWjKTOBhUYL0MRX4M3oQ.png" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Basic neural network from <a class="reference external" href="https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9">https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9</a>.</span><a class="headerlink" href="#neuralnet-basic-fig" title="Permalink to this image">¶</a></p>
</div>
<p>In network architectures, neurons are grouped in layers, with synapses traversing the interstitial space between neurons in one layer and the next.</p>
<div class="section" id="what-are-convolutional-neural-networks">
<h4>What are Convolutional Neural Networks?<a class="headerlink" href="#what-are-convolutional-neural-networks" title="Permalink to this headline">¶</a></h4>
<p>A Convolutional Neural Network (ConvNet/CNN) is a form of deep learning inspired by the organization of the human visual cortex, in which individual neurons respond to stimuli within a constrained region of the visual field known as the receptive field. Several receptive fields overlap to account for the entire visual area.</p>
<p>In artificial CNNs, an input matrix such as an image is given importance per various aspects and objects in the image through a moving, convoling receptive field. Very little pre-processing is required for CNNs relative to other classification methods as the need for upfront feature-engineering is removed. Rather, CNNs learn the correct filters and consequent features on their own, provided enough training time and examples.</p>
<div class="figure align-default" id="convolution-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif"><img alt="https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif" src="https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Convolution of a kernal over an input matrix from <a class="reference external" href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1">https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1</a>.</span><a class="headerlink" href="#convolution-fig" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="what-is-a-kernel-filter">
<h4>What is a kernel/filter?<a class="headerlink" href="#what-is-a-kernel-filter" title="Permalink to this headline">¶</a></h4>
<p>A kernel is matrix smaller than the input. It acts as a receptive field that moves over the input matrix from left to right and top to bottom and filters for features in the image.</p>
</div>
<div class="section" id="what-is-stride">
<h4>What is stride?<a class="headerlink" href="#what-is-stride" title="Permalink to this headline">¶</a></h4>
<p>Stride refers to the number of pixels that the kernel shifts at each step in its navigation of the input matrix.</p>
</div>
<div class="section" id="what-is-a-convolution-operation">
<h4>What is a convolution operation?<a class="headerlink" href="#what-is-a-convolution-operation" title="Permalink to this headline">¶</a></h4>
<p>The convolution operation is the combination of two functions to produce a third function as a result. In effect, it is a merging of two sets of information, the kernel and the input matrix.</p>
<div class="figure align-default" id="convolution-arithmetic-fig">
<a class="reference internal image-reference" href="https://theano-pymc.readthedocs.io/en/latest/_images/numerical_no_padding_no_strides.gif"><img alt="https://theano-pymc.readthedocs.io/en/latest/_images/numerical_no_padding_no_strides.gif" src="https://theano-pymc.readthedocs.io/en/latest/_images/numerical_no_padding_no_strides.gif" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Convolution of a kernal over an input matrix from <a class="reference external" href="https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html">https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html</a>.</span><a class="headerlink" href="#convolution-arithmetic-fig" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="convolution-operation-using-3d-filter">
<h4>Convolution operation using 3D filter<a class="headerlink" href="#convolution-operation-using-3d-filter" title="Permalink to this headline">¶</a></h4>
<p>An input image is often represented as a 3D matrix with a dimension for width (pixels), height (pixels), and depth (channels). In the case of an optical image with red, green and blue channels, the kernel/filter matrix is shaped with the same channel depth as the input and the weighted sum of dot products is computed across all 3 dimensions.</p>
</div>
<div class="section" id="what-is-padding">
<h4>What is padding?<a class="headerlink" href="#what-is-padding" title="Permalink to this headline">¶</a></h4>
<p>After a convolution operation, the feature map is by default smaller than the original input matrix.</p>
<div class="figure align-default" id="multi-layer-cnn-fig">
<a class="reference internal image-reference" href="https://www.researchgate.net/profile/Sheraz-Khan-14/publication/321586653/figure/fig4/AS:568546847014912&#64;1512563539828/The-LeNet-5-Architecture-a-convolutional-neural-network.png"><img alt="https://www.researchgate.net/profile/Sheraz-Khan-14/publication/321586653/figure/fig4/AS:568546847014912&#64;1512563539828/The-LeNet-5-Architecture-a-convolutional-neural-network.png" src="https://www.researchgate.net/profile/Sheraz-Khan-14/publication/321586653/figure/fig4/AS:568546847014912&#64;1512563539828/The-LeNet-5-Architecture-a-convolutional-neural-network.png" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text"><a class="reference external" href="https://www.researchgate.net/figure/The-LeNet-5-Architecture-a-convolutional-neural-network_fig4_321586653">Progressive downsizing of feature maps in a multi-layer CNN</a>.</span><a class="headerlink" href="#multi-layer-cnn-fig" title="Permalink to this image">¶</a></p>
</div>
<p>To maintain the same spatial dimensions between input matrix and output feature map, we may pad the input matrix with a border of zeroes or ones. There are two types of padding:</p>
<ol class="simple">
<li><p>Same padding: a border of zeroes or ones is added to match the input/output dimensions</p></li>
<li><p>Valid padding: no border is added and the output dimensions are not matched to the input</p></li>
</ol>
<div class="figure align-default" id="padding-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/666/1*noYcUAa_P8nRilg3Lt_nuA.png"><img alt="https://miro.medium.com/max/666/1*noYcUAa_P8nRilg3Lt_nuA.png" src="https://miro.medium.com/max/666/1*noYcUAa_P8nRilg3Lt_nuA.png" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text"><a class="reference external" href="https://ayeshmanthaperera.medium.com/what-is-padding-in-cnns-71b21fb0dd7">Padding an input matrix with zeroes</a>.</span><a class="headerlink" href="#padding-fig" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="what-is-deep-learning">
<h3>What is Deep Learning?<a class="headerlink" href="#what-is-deep-learning" title="Permalink to this headline">¶</a></h3>
<p>Deep learning is defined by neural networks with depth, i.e. many layers and connections. The reason for why deep learning is so highly performant lies in the degree of abstraction made possible by feature extraction across so many layers in which each neuron, or processing unit, is interacting with input from neurons in previous layers and making decisions accordingly. The deepest layers of a network once trained can be capable inferring highly abstract concepts, such as what differentiates a school from a house in satellite imagery.</p>
<div class="admonition-cost-of-deep-learning admonition">
<p class="admonition-title"><strong>Cost of deep learning</strong></p>
<p>Deep learning requires a lot of data to learn from and usually a significant amount of computing power, so it can be expensive depending on the scope of the problem.</p>
</div>
<div class="section" id="training-and-testing-data">
<h4>Training and Testing Data<a class="headerlink" href="#training-and-testing-data" title="Permalink to this headline">¶</a></h4>
<p>The dataset (e.g. all images and their labels) are split into training, validation and testing sets. A common ratio is 70:20:10 percent, train:validation:test.  If randomly split, it is important to check that all class labels exist in all sets and are well represented.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Why do we need validation and test data? Are they redundant?
We need separate test data to evaluate the performance of the model because the validation data is used during training to measure error and therefore inform updates to the model parameters. Therefore, validation data is not unbiased to the model. A need for new, wholly unseen data to test with is required.</p>
</div>
</div>
<div class="section" id="forward-and-backward-propagation-hyper-parameters-and-learnable-parameters">
<h4>Forward and backward propagation, hyper-parameters, and learnable parameters<a class="headerlink" href="#forward-and-backward-propagation-hyper-parameters-and-learnable-parameters" title="Permalink to this headline">¶</a></h4>
<p>Neural networks train in cycles, where the input data passes through the network, a relationship between input data and target values is learned, a prediction is made, the prediction value is measured for error relative to its true value, and the errors are used to inform updates to parameters in the network, feeding into the next cycle of learning and prediction using the updated information. This happens through a two-step process called forward propagation and back propagation, in which the first part is used to gather knowledge and the second part is used to correct errors in the model’s knowledge.</p>
<div class="figure align-default" id="forward-backprop-fig">
<a class="reference internal image-reference" href="https://thumbs.gfycat.com/BitesizedWeeBlacklemur-max-1mb.gif"><img alt="https://thumbs.gfycat.com/BitesizedWeeBlacklemur-max-1mb.gif" src="https://thumbs.gfycat.com/BitesizedWeeBlacklemur-max-1mb.gif" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text"><a class="reference external" href="https://gfycat.com/gifs/search/backpropagation">Forward and back propagation</a>.</span><a class="headerlink" href="#forward-backprop-fig" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>activation function</strong> decides whether or not the output from one neuron is useful or not based on a threshold value, and therefore, whether it will be carried from one layer to the next.</p>
<p><strong>Weights</strong> control the signal (or the strength of the connection) between two neurons in two consecutive layers.</p>
<p><strong>Biases</strong> are values which help determine whether or not the activation output from a neuron is going to be passed forward through the network.</p>
<p>In a neural network, neurons in one layer are connected to neurons in the next layer. As information passes from one neuron to the next, the information is conditioned by the weight of the synapse and is subjected to a bias. The weights and biases determine if the information passes further beyond the current neuron.</p>
<div class="figure align-default" id="activation-fig">
<a class="reference internal image-reference" href="https://cdn-images-1.medium.com/max/651/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg"><img alt="https://cdn-images-1.medium.com/max/651/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg" src="https://cdn-images-1.medium.com/max/651/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text"><a class="reference external" href="https://laptrinhx.com/statistics-is-freaking-hard-wtf-is-activation-function-207913705/">Weights, bias, activation</a>.</span><a class="headerlink" href="#activation-fig" title="Permalink to this image">¶</a></p>
</div>
<p>During training, the weights and biases are learned and updated using the training and validation dataset to fit the data and reduce error of prediction values relative to target values.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<ul class="simple">
<li><p><strong>Activation function</strong>: decides whether or not the output from one neuron is useful or not</p></li>
<li><p><strong>Weights</strong>: control the signal between neurons in consecutive layers</p></li>
<li><p><strong>Biases</strong>: a threshold value that determines the activation of each neuron</p></li>
<li><p>Weights and biases are the learnable parameters of a deep learning model</p></li>
</ul>
</div>
<p>The <strong>learning rate</strong> controls how much we want the model to change in response to the estimated error after each training cycle</p>
<div class="figure align-default" id="loss-curve-fig">
<a class="reference internal image-reference" href="https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/09/Neural-network-32-i2tutorials.png"><img alt="https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/09/Neural-network-32-i2tutorials.png" src="https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/09/Neural-network-32-i2tutorials.png" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text"><a class="reference external" href="https://www.i2tutorials.com/what-are-local-minima-and-global-minima-in-gradient-descent/">Local vs. global minimum (the optimal point to reach)</a>.</span><a class="headerlink" href="#loss-curve-fig" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>batch size</strong> determines the portion of our training dataset that can be fed to the model during each cycle. Stated otherwise, batch size controls the number of training samples to work through before the model’s internal parameters are updated.</p>
<div class="figure align-default" id="batch-epoch-fig">
<a class="reference internal image-reference" href="https://www.baeldung.com/wp-content/uploads/sites/4/2020/12/epoch-batch-size.png"><img alt="https://www.baeldung.com/wp-content/uploads/sites/4/2020/12/epoch-batch-size.png" src="https://www.baeldung.com/wp-content/uploads/sites/4/2020/12/epoch-batch-size.png" style="width: 250px;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text"><a class="reference external" href="https://www.baeldung.com/cs/epoch-neural-networks">Modulating batch size detetmines how many iterations are within one epoch</a>.</span><a class="headerlink" href="#batch-epoch-fig" title="Permalink to this image">¶</a></p>
</div>
<p>An <strong>epoch</strong> is defined as the point when all training samples, aka the entire dataset, has passed through the neural network once. The number of epochs controls how many times the entire dataset is cycled through and analyzed by the neural network. Related, but not necessarily as a parameter is an <strong>iteration</strong>, which is the pass of one batch through the network. If the batch size is smaller than the size of the whole dataset, then there are multiple iterations in one epoch.</p>
<p>The <strong>optimization function</strong> is really important. It’s what we use to change the attributes of your neural network such as weights and biases in order to reduce the losses. The goal of an optimization function is to minimize the error produced by the model.</p>
<p>The <strong>loss function</strong>, also known as the cost function, measures how much the model needs to improve based on the prediction errors relative to the true values during training.</p>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="https://miro.medium.com/max/810/1*UUHvSixG7rX2EfNFTtqBDA.gif"><img alt="https://miro.medium.com/max/810/1*UUHvSixG7rX2EfNFTtqBDA.gif" src="https://miro.medium.com/max/810/1*UUHvSixG7rX2EfNFTtqBDA.gif" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text"><a class="reference external" href="https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220">Loss curve</a>.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>accuracy metric</strong> measures the performance of a model. For example, a pixel to pixel comparison for agreement on class.</p>
<p>Note: the <strong>activation function</strong> is also a hyper-parameter.</p>
</div>
<div class="section" id="common-deep-learning-algorithms-for-computer-vision">
<h4>Common Deep Learning Algorithms for Computer Vision<a class="headerlink" href="#common-deep-learning-algorithms-for-computer-vision" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Image classification: classifying whole images, e.g. image with clouds, image without clouds</p></li>
<li><p>Object detection: identifying locations of objects in an image and classifying them, e.g. identify bounding boxes of cars and planes in satellite imagery</p></li>
<li><p>Semantic segmentation: classifying individual pixels in an image, e.g. land cover classification</p></li>
<li><p>Instance segmentation: classifying individual pixels in an image in terms of both class and individual membership, e.g. detecting unique agricultural field polygons and classifying them</p></li>
<li><p>Generative Adversarial:  a type of image generation where synthetic images are created from real ones, e.g. creating synthetic landscapes from real landscape images</p></li>
</ul>
</div>
<div class="section" id="semantic-segmentation">
<h4>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this headline">¶</a></h4>
<p>To pair with the content of these tutorials, we will demonstrate semantic segmentation (supervised) to map land use categories and illegal gold mining activity.</p>
<ul class="simple">
<li><p>Semantic = of or relating to meaning (class)</p></li>
<li><p>Segmentation = division (of image) into separate parts</p></li>
</ul>
</div>
<div class="section" id="u-net-segmentation-architecture">
<h4>U-Net Segmentation Architecture<a class="headerlink" href="#u-net-segmentation-architecture" title="Permalink to this headline">¶</a></h4>
<p>Semantic segmentation is often distilled into the combination of an encoder and a decoder. An encoder generates logic or feedback from input data, and a decoder takes that feedback and translates it to output data in the same form as the input.</p>
<p>The U-Net model, which is one of many deep learning segmentation algorithms, has a great illustration of this structure.</p>
<div class="figure align-default" id="unet-fig">
<a class="reference internal image-reference" href="https://developers.arcgis.com/assets/img/python-graphics/unet.png"><img alt="https://developers.arcgis.com/assets/img/python-graphics/unet.png" src="https://developers.arcgis.com/assets/img/python-graphics/unet.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">U-Net architecture (from <a class="reference external" href="https://arxiv.org/abs/1505.04597">Ronneberger et al., 2015</a>).</span><a class="headerlink" href="#unet-fig" title="Permalink to this image">¶</a></p>
</div>
<p>In Fig. 13, the encoder is on the left side of the model. It consists of consecutive convolutional layers, each followed by ReLU and a max pooling operation to encode feature representations at multiple scales. The encoder can be represented by most feature extraction networks designed for classification.</p>
<p>The decoder, on the right side of the Fig. 13 diagram, is tasked to semantically project the discriminative features learned by the encoder onto the original pixel space to render a dense classification. The decoder consists of deconvolution and concatenation followed by regular convolution operations.</p>
<p>Following the decoder is the final classification layer, which computes the pixel-wise classification for each cell in the final feature map.</p>
<p>ReLU is an operation, an activation function to be specific, that induces non-linearity. This function intakes the feature map from a convolution operation and remaps it such that any positive value stays exactly the same, and any negative value becomes zero.</p>
<div class="figure align-default" id="relu-graph-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/3200/1*w48zY6o9_5W9iesSsNabmQ.gif"><img alt="https://miro.medium.com/max/3200/1*w48zY6o9_5W9iesSsNabmQ.gif" src="https://miro.medium.com/max/3200/1*w48zY6o9_5W9iesSsNabmQ.gif" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text"><a class="reference external" href="https://medium.com/ai%C2%B3-theory-practice-business/magic-behind-activation-function-c6fbc5e36a92">ReLU activation function</a>.</span><a class="headerlink" href="#relu-graph-fig" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="relu-maxpooling-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/1000/1*cmGESKfSZLH2ksqF_kBgfQ.gif"><img alt="https://miro.medium.com/max/1000/1*cmGESKfSZLH2ksqF_kBgfQ.gif" src="https://miro.medium.com/max/1000/1*cmGESKfSZLH2ksqF_kBgfQ.gif" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text"><a class="reference external" href="https://towardsdatascience.com/a-laymans-guide-to-building-your-first-image-classification-model-in-r-using-keras-b285deac6572">ReLU applied to an input matrix</a>.</span><a class="headerlink" href="#relu-maxpooling-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Max pooling is used to summarize a feature map and only retain the important structural elements, foregoing the more granular detail that may not be significant to the modeling task. This helps to denoise the signal and helps with computational efficiency. It works similar to convolution in that a kernel with a stride is applied to the feature map and only the maximum value within each patch is reserved.</p>
<div class="figure align-default" id="maxpooling-fig">
<a class="reference internal image-reference" href="https://thumbs.gfycat.com/FirstMediumDalmatian-size_restricted.gif"><img alt="https://thumbs.gfycat.com/FirstMediumDalmatian-size_restricted.gif" src="https://thumbs.gfycat.com/FirstMediumDalmatian-size_restricted.gif" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 16 </span><span class="caption-text"><a class="reference external" href="https://gfycat.com/firstmediumdalmatian">Max pooling with a kernal over an input matrix</a>.</span><a class="headerlink" href="#maxpooling-fig" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="index.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Deep Learning with TensorFlow: <br> Tutorials for modeling in the Amazon</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Lesson1b_Intro_TensorFlow_Keras.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Introduction to TensorFlow and Keras</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Development Seed<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>